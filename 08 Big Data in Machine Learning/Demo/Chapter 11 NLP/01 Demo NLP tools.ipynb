{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "painted-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "technical-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prompt-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egyptian-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proud-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('association_rule').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ignored-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I know Spark can work well with NLP\"),\n",
    "    (2, \"Logistic,regession,models,are,supervised\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-dressing",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reverse-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pacific-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------------------------------------+------+\n",
      "|sentence                                |words                                       |tokens|\n",
      "+----------------------------------------+--------------------------------------------+------+\n",
      "|Hi I heard about Spark                  |[hi, i, heard, about, spark]                |5     |\n",
      "|I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]|8     |\n",
      "|Logistic,regession,models,are,supervised|[logistic,regession,models,are,supervised]  |1     |\n",
      "+----------------------------------------+--------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\W+\", gap(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select('sentence', 'words').withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flying-nation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------------+------+\n",
      "|sentence                                |words                                         |tokens|\n",
      "+----------------------------------------+----------------------------------------------+------+\n",
      "|Hi I heard about Spark                  |[hi, i, heard, about, spark]                  |5     |\n",
      "|I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]  |8     |\n",
      "|Logistic,regession,models,are,supervised|[logistic, regession, models, are, supervised]|5     |\n",
      "+----------------------------------------+----------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select('sentence', 'words').withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-lawsuit",
   "metadata": {},
   "source": [
    "### Stopword Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "educated-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fluid-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+---------------------+\n",
      "|id |raw                           |filtered             |\n",
      "+---+------------------------------+---------------------+\n",
      "|0  |[I, go, to, school, by, bus]  |[go, school, bus]    |\n",
      "|1  |[Minh, has, lots, of, pencils]|[Minh, lots, pencils]|\n",
      "+---+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([(0, ['I', \"go\", \"to\", \"school\", \"by\", \"bus\"]),\n",
    "                                      (1, ['Minh', \"has\", 'lots', 'of', \"pencils\"])], \n",
    "                                     [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol = 'raw', outputCol = 'filtered')\n",
    "remover.transform(sentenceData).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-cross",
   "metadata": {},
   "source": [
    "### NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "victorian-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invisible-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "|sentence                                |words                                         |ngrams                                                                   |\n",
      "+----------------------------------------+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "|Hi I heard about Spark                  |[hi, i, heard, about, spark]                  |[hi i, i heard, heard about, about spark]                                |\n",
      "|I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]  |[i know, know spark, spark can, can work, work well, well with, with nlp]|\n",
      "|Logistic,regession,models,are,supervised|[logistic, regession, models, are, supervised]|[logistic regession, regession models, models are, are supervised]       |\n",
      "+----------------------------------------+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=2, inputCol='words', outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(regexTokenized)\n",
    "ngramDataFrame.select('sentence', 'words', 'ngrams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-offer",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hollow-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accurate-boards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+\n",
      "|id |sentence                                |words                                         |rawFeatures                           |\n",
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+\n",
      "|0  |Hi I heard about Spark                  |[hi, i, heard, about, spark]                  |(10,[3,6,8],[1.0,3.0,1.0])            |\n",
      "|1  |I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]  |(10,[0,3,6,7,9],[1.0,1.0,2.0,2.0,2.0])|\n",
      "|2  |Logistic,regession,models,are,supervised|[logistic, regession, models, are, supervised]|(10,[1,3,4,5,8],[1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=10)\n",
    "featureizedData = hashingTF.transform(regexTokenized)\n",
    "featureizedData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raising-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "swedish-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                           |\n",
      "+---+---------------------------------------------------------------------------------------------------+\n",
      "|0  |(10,[3,6,8],[0.0,0.8630462173553426,0.28768207245178085])                                          |\n",
      "|1  |(10,[0,3,6,7,9],[0.6931471805599453,0.0,0.5753641449035617,1.3862943611198906,1.3862943611198906]) |\n",
      "|2  |(10,[1,3,4,5,8],[0.6931471805599453,0.0,0.6931471805599453,0.6931471805599453,0.28768207245178085])|\n",
      "+---+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(featureizedData)\n",
    "rescaledData = idfModel.transform(featureizedData)\n",
    "\n",
    "rescaledData.select('id', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-premiere",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "arctic-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "personal-donor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+-------------------------+\n",
      "|id |sentence                                |words                                         |rawFeatures                           |features_C               |\n",
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+-------------------------+\n",
      "|0  |Hi I heard about Spark                  |[hi, i, heard, about, spark]                  |(10,[3,6,8],[1.0,3.0,1.0])            |(4,[0,1,3],[1.0,1.0,1.0])|\n",
      "|1  |I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]  |(10,[0,3,6,7,9],[1.0,1.0,2.0,2.0,2.0])|(4,[0,1],[1.0,1.0])      |\n",
      "|2  |Logistic,regession,models,are,supervised|[logistic, regession, models, are, supervised]|(10,[1,3,4,5,8],[1.0,1.0,1.0,1.0,1.0])|(4,[2],[1.0])            |\n",
      "+---+----------------------------------------+----------------------------------------------+--------------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a CountVectorizerModel from the corpus\n",
    "# vocabSize: Số lượng từ duy nhất\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features_C\", vocabSize=4, minDF=1)\n",
    "model = cv.fit(featureizedData)\n",
    "result = model.transform(featureizedData)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-massage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
